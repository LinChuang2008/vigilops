# 告警疲劳：你的监控系统正在浪费你多少时间？

> 作者：VigilOps 团队 | 2026-02

---

## 你是不是也这样？

每天打开钉钉，置顶群"运维告警"未读 99+。滑一遍，CPU 高了、磁盘快满了、某个服务响应慢了、某个 Pod 重启了……大部分看了标题就直接划走，因为你知道这些"告警"其实不需要你做什么。

但偶尔，真正的问题就混在这堆噪音里。等你注意到的时候，用户已经在群里投诉了。

这不是你的错。这是整个运维行业都在面对的问题——**告警疲劳（Alert Fatigue）**。

## 告警疲劳不是新话题

2018 年，PagerDuty 发布了一份关于运维人员职业倦怠的报告。其中一个数据点被广泛引用：频繁的告警中断会导致 on-call 工程师的注意力和判断力显著下降。这个结论在运维圈并不令人意外——任何做过值班的人都有体会。

医疗行业更早意识到这个问题。美国 ECRI Institute 曾多次将"告警疲劳"列为患者安全的十大威胁之一。ICU 病房里的监护仪每天会产生几百次告警，护士被迫忽略大部分，直到真正的危险信号也被忽略。

运维监控面临同样的困境：**当一切都是"紧急"的时候，就没有什么是紧急的。**

## 为什么告警会越来越多？

这不难理解。一个典型的成长过程：

**第一阶段：谨慎配置。** 刚搭好 Prometheus，你只设了几个关键告警——CPU > 90%、磁盘 > 85%、服务挂了。告警很少，每条都值得关注。

**第二阶段：恐惧驱动。** 出了一次线上事故，因为某个指标没有告警覆盖。于是你加了更多规则，阈值也调低了。"宁可多报不能漏报"。

**第三阶段：噪音洪流。** 告警数量从每天几条变成几十条，再到上百条。微服务扩容后，每个实例都在独立告警。告警群变成了最不想打开的钉钉群。

**第四阶段：习得性无助。** 你开始条件反射地忽略告警。甚至新来的同事也很快学会了这一点。直到某天真的出了事，大家面面相觑——"那条告警昨天就有了？"

如果这个过程听起来很熟悉，你不是个例。这是运维团队普遍的演化路径。

## 现有工具解决了什么，没解决什么

### Prometheus AlertManager

AlertManager 提供了分组（grouping）、抑制（inhibition）、静默（silence）功能。分组可以把同一类告警合并，减少通知数量。但它的局限也很明显：

- 分组规则需要手动配置，且依赖标签（label）一致性
- 无法判断一个告警是否"真的需要人关注"
- 没有根因分析能力——它只知道阈值触发了，不知道为什么

### Grafana OnCall

Grafana OnCall 解决了告警路由和排班问题（谁来接这个告警），但不解决"这个告警是否值得接"的问题。它让正确的人收到告警，但不减少告警本身。

### PagerDuty

PagerDuty 的 Event Intelligence 确实在做告警降噪，包括 ML 驱动的告警聚合。但它是一个独立的付费产品，需要和你的监控系统集成，且价格不低（按用户计费，Teams 方案 $29/用户/月起）。

### 缺了什么？

一个集成在监控系统里的、能理解上下文的、能自动判断告警重要性的智能层。不是又一个需要对接的外部工具，而是监控系统本身就应该具备的能力。

## VigilOps 的做法：AI 分析 + 自动修复

我们在 VigilOps 里尝试了一个不同的思路：让 AI（基于 DeepSeek）参与告警处理的全流程。

### 告警触发后会发生什么

```
1. 告警规则触发（和普通监控一样）
   ↓
2. AI 分析引擎介入：
   - 读取告警关联的指标数据和最近的日志
   - 分析是否和其他活跃告警相关
   - 判断根因和严重程度
   ↓
3. 如果匹配内置 Runbook：
   - 安全检查（确认 Runbook 适用于当前场景）
   - 执行自动修复
   - 记录执行结果
   ↓
4. 如果不匹配 Runbook：
   - 将 AI 分析结果附在告警上
   - 正常通知值班人员
```

### 一个具体例子

假设你收到告警：**"服务器 web-03 磁盘使用率达到 93%"**。

在传统流程中，值班工程师收到通知 → 登录服务器 → 检查是哪个目录占用大 → 清理临时文件或旧日志 → 确认恢复。整个过程 15-30 分钟。

在 VigilOps 中：

1. AI 分析发现 `/tmp` 和 `/var/log` 目录增长异常，关联到最近的日志写入量增加
2. 匹配 `disk_cleanup` Runbook
3. 自动清理 7 天前的临时文件和已轮转的日志
4. 磁盘使用率降到 62%，告警自动解除
5. 执行记录可在审计日志中查看

值班工程师第二天上班看到的是一条"已自动修复"的记录，而不是一条需要处理的告警。

### 自己动手试一下

```bash
# 克隆项目
git clone https://github.com/LinChuang2008/vigilops.git
cd vigilops

# 配置环境
cp .env.example .env
# 编辑 .env，填入你的 DeepSeek API Key

# 启动
docker compose up -d

# 打开浏览器
# http://localhost:3001
```

或者直接访问在线 Demo：[http://139.196.210.68:3001](http://139.196.210.68:3001)（账号 `demo@vigilops.io` / `demo123`，只读）

在 Demo 中，你可以：
- 查看告警列表，注意 AI 分析结果字段
- 进入 Runbook 页面，查看 6 个内置修复脚本的逻辑
- 查看审计日志，了解自动修复的执行记录

## 告警疲劳的根本解法

技术方案只是一部分。根本上，减少告警疲劳需要三个层面的改变：

**1. 减少无效告警的产生。** 审视你的告警规则，问自己：如果这条告警触发了，需要有人立即做什么吗？如果答案是"不一定"，它不应该是告警，应该是一个日志或指标面板上的信息。

**2. 让机器处理机器能处理的事。** 磁盘清理、服务重启、日志轮转——这些是确定性的操作，不需要高级工程师半夜起来执行。自动修复不是取代运维，而是把运维的时间留给真正需要人类判断的问题。

**3. 建立告警治理文化。** 把"告警数量"作为团队的一个健康指标来跟踪。每月做一次告警回顾：上个月哪些告警最多？哪些从来没有被人处理？哪些可以合并或删除？

## 写在最后

VigilOps 是一个早期项目。我们不会假装它能解决所有告警疲劳问题——告警疲劳的根源既有技术层面，也有组织层面。

但我们相信一个方向：**监控系统不应该只会喊"着火了"，它也应该会灭火。** 至少对于那些常见的、可预期的问题，自动修复是一个值得尝试的解决方案。

如果你也被告警疲劳困扰，欢迎试用 VigilOps，或者在 [GitHub Discussions](https://github.com/LinChuang2008/vigilops/discussions) 和我们交流你的经验。我们特别想听到：你的告警中，有多少比例是"看了但不需要操作的"？

---

*VigilOps 是一个 Apache 2.0 开源项目。代码在 [GitHub](https://github.com/LinChuang2008/vigilops)。*
